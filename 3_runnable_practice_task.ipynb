{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04d30f0b",
   "metadata": {},
   "source": [
    "## RunnableLambda, RunnableSequence and RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38972080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere import ChatCohere\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableSequence, RunnableLambda, RunnableParallel\n",
    "from langchain_google_genai.chat_models import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0b360ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_settings import BaseSettings\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    \n",
    "    cohere_api_key: str\n",
    "    google_api_key: str\n",
    "\n",
    "    class Config:\n",
    "        env_file = \"../.env\"\n",
    "        extra = \"ignore\"\n",
    "        \n",
    "settings = Settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32c8e899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(model='models/gemini-2.5-flash-lite', google_api_key=SecretStr('**********'), temperature=0.0, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x00000231609DDD30>, default_metadata=(), model_kwargs={})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model = \"gemini-2.5-flash-lite\",\n",
    "    google_api_key = settings.google_api_key,\n",
    "    temperature = 0\n",
    ")\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673117a5",
   "metadata": {},
   "source": [
    "### 1. Multi-Step Sentiment Pipeline\n",
    "#### Tags: RunnableSequence, RunnableLambda\n",
    "#### Task: Create a sequence where:\n",
    "1. Input is customer feedback text.\n",
    "2. Step 1: Detect language (lambda).\n",
    "3. Step 2: Translate to English if needed.\n",
    "4. Step 3: Analyze sentiment.\n",
    "5. Step 4: Output language + sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f9e2960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['feedback'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are the customer feedback text analyzer.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['feedback'], input_types={}, partial_variables={}, template='{feedback}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are the customer feedback text analyzer.\"), \n",
    "        (\"human\", \"{feedback}\")\n",
    "    ]\n",
    ")\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ce7eb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_chain = RunnableSequence(\n",
    "#     prompt_template\n",
    "#     | llm \n",
    "#     | StrOutputParser()\n",
    "# )\n",
    "# prompt_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b55af09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3172771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_language = RunnableLambda(\n",
    "    lambda x: {\n",
    "        \"text\": x[\"feedback\"],\n",
    "        \"language\": detect(x[\"feedback\"])\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5674646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect_chain = RunnableSequence(\n",
    "#     prompt_chain\n",
    "#     | llm\n",
    "#     | detect_language\n",
    "# )\n",
    "# detect_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c03f162b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=[], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are the feedback analyzer. If the human provides text in any language, translate it into English if not already in English.'), additional_kwargs={})])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_translation = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are the feedback analyzer. If the human provides text in any language, translate it into English if not already in English.\"),\n",
    "        # (\"human\", \"{feedback}\")\n",
    "    ]\n",
    ")\n",
    "language_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dfc82b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=[], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are the feedback analyzer. If the human provides text in any language, translate it into English if not already in English.'), additional_kwargs={})])\n",
       "| ChatGoogleGenerativeAI(model='models/gemini-2.5-flash-lite', google_api_key=SecretStr('**********'), temperature=0.0, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x00000231609DDD30>, default_metadata=(), model_kwargs={})\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_chain = (\n",
    "    language_translation \n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "translate_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "338737c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are the sentiment analyzer. Read the following feedback and classify it as: **Positive**, **Negative**, and **Neutral**..'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='{text}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_sentiment = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are the sentiment analyzer. Read the following feedback and classify it as: **Positive**, **Negative**, and **Neutral**..\"),\n",
    "        (\"human\", \"{text}\")\n",
    "    ]\n",
    ")\n",
    "analyze_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "386214b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are the sentiment analyzer. Read the following feedback and classify it as: **Positive**, **Negative**, and **Neutral**..'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='{text}'), additional_kwargs={})])\n",
       "| ChatGoogleGenerativeAI(model='models/gemini-2.5-flash-lite', google_api_key=SecretStr('**********'), temperature=0.0, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x00000231609DDD30>, default_metadata=(), model_kwargs={})\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_chain = (\n",
    "    analyze_sentiment\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "analyze_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a4824d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableSequence(\n",
    "    prompt_template\n",
    "    | detect_language\n",
    "    | RunnableLambda(lambda x: {\"feedback\": x[\"feedback\"]}) \n",
    "    | translate_chain\n",
    "    | RunnableLambda(lambda t: {\"english_text\": t})\n",
    "    | analyze_chain\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5458754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine_chain = RunnableSequence(\n",
    "#     detect_language\n",
    "#     | RunnableLambda(lambda x: {\"feedback\": x[\"feedback\"], \"language\": x[\"language\"], \"text\": x[\"feedback\"]})\n",
    "#     | translate_chain\n",
    "#     | RunnableLambda(lambda x: {**x, \"english_text\": x[\"english_text\"]})\n",
    "#     | analyze_chain\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df901d5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ChatPromptValue' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfeedback\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mഞാൻ ഒരു സ്മാർട്ട്ഫോൺ വാങ്ങി. അതിന്റെ ഗുണനിലവാരവും സവിശേഷതകളും നല്ലതാണ്.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VRNeXGen\\my_ai_env\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3049\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3047\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3048\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3049\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3050\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3051\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VRNeXGen\\my_ai_env\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4781\u001b[39m, in \u001b[36mRunnableLambda.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   4767\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Invoke this Runnable synchronously.\u001b[39;00m\n\u001b[32m   4768\u001b[39m \n\u001b[32m   4769\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4778\u001b[39m \u001b[33;03m    TypeError: If the Runnable is a coroutine function.\u001b[39;00m\n\u001b[32m   4779\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4780\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfunc\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m4781\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4782\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4783\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4784\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4785\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4786\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4787\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mCannot invoke a coroutine function synchronously.Use `ainvoke` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4788\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VRNeXGen\\my_ai_env\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1938\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1934\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1935\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1936\u001b[39m         output = cast(\n\u001b[32m   1937\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1938\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1939\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1940\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1944\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1945\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1946\u001b[39m         )\n\u001b[32m   1947\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1948\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VRNeXGen\\my_ai_env\\Lib\\site-packages\\langchain_core\\runnables\\config.py:429\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    428\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VRNeXGen\\my_ai_env\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4639\u001b[39m, in \u001b[36mRunnableLambda._invoke\u001b[39m\u001b[34m(self, input_, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m   4637\u001b[39m                 output = chunk\n\u001b[32m   4638\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4639\u001b[39m     output = \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4640\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   4641\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4642\u001b[39m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[32m   4643\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VRNeXGen\\my_ai_env\\Lib\\site-packages\\langchain_core\\runnables\\config.py:429\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    428\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m      1\u001b[39m detect_language = RunnableLambda(\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m x: {\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfeedback\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[32m      4\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlanguage\u001b[39m\u001b[33m\"\u001b[39m: detect(x[\u001b[33m\"\u001b[39m\u001b[33mfeedback\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      5\u001b[39m     }\n\u001b[32m      6\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: 'ChatPromptValue' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"feedback\": \"ഞാൻ ഒരു സ്മാർട്ട്ഫോൺ വാങ്ങി. അതിന്റെ ഗുണനിലവാരവും സവിശേഷതകളും നല്ലതാണ്.\"\n",
    "    }\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68982b55",
   "metadata": {},
   "source": [
    "### 2. Parallel News Analysis\n",
    "#### Tags: RunnableParallel\n",
    "#### Task: Given a news article, run three processes in parallel:\n",
    "- Summarization\n",
    "- Extract keywords\n",
    "- Detect political bias\n",
    "\n",
    "Return all three results in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3887ef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are the summarization assistant for news article to summerize the content. output is form a structure\"),\n",
    "        (\"human\", \"summarize the following article {article}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "summarization_chain = summarization_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3560e580",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_keyword_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are the expect of extract keyword assistant to the new article to extract the important keyword.\"),\n",
    "        (\"human\", \" Extract the keywords {article}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "extract_keyword_chain = extract_keyword_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52e8fa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_political_bias_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are expert the media analyst. Read the following article and detect any political bias. Classify it as: Pro-Government, Anti-Government, Neutral, or Other.\"),\n",
    "        (\"human\", \"{article}\")\n",
    "    ]\n",
    ")\n",
    "detect_political_bias_chain = detect_political_bias_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cae78a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_chain: dict = RunnableParallel(\n",
    "    summarization = summarization_chain,\n",
    "    extract_keyword= extract_keyword_chain,\n",
    "    detect_political_bias = detect_political_bias_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d326626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summarization': \"**Summary of Infrastructure Development Program**\\n\\n*   **Key Benefit:** Boosts economic growth and creates thousands of jobs.\\n*   **Expert Opinion:** Expected to improve transportation, connectivity, and local businesses, strengthening the national economy.\\n*   **Public Reception:** Widely welcomed by the majority of citizens, who praise the government's decisive action towards modernization and national progress.\\n*   **Opposition View:** Minor concerns have been raised by opposition parties.\",\n",
       " 'extract_keyword': \"Here are the keywords extracted from the article:\\n\\n*   infrastructure development program\\n*   economic growth\\n*   job creation\\n*   transportation\\n*   connectivity\\n*   local businesses\\n*   nation's economy\\n*   modernization\\n*   national progress\",\n",
       " 'detect_political_bias': '**Classification:** Pro-Government\\n\\n**Analysis:**\\n\\nThe article presents a overwhelmingly positive view of the government\\'s infrastructure development program. Here\\'s why it leans towards \"Pro-Government\":\\n\\n*   **Positive Framing:** Words like \"widespread acclaim,\" \"boosting economic growth,\" \"creating thousands of jobs,\" \"improve transportation, connectivity, and local businesses,\" and \"strengthening the nation\\'s economy\" all carry a positive connotation and highlight the benefits of the program.\\n*   **Expert Endorsement:** The inclusion of \"Experts say...\" lends credibility and authority to the positive claims about the program\\'s impact.\\n*   **Minimizing Opposition:** Opposition concerns are described as \"minor,\" which downplays their significance and suggests they are not substantial enough to detract from the program\\'s overall success.\\n*   **Public Approval:** The statement that \"the majority of citizens have welcomed the initiative, praising the government...\" directly attributes positive sentiment and praise to the government.\\n*   **Focus on Government Action:** The article emphasizes the government \"taking decisive steps toward modernization and national progress,\" framing the government as proactive and effective.\\n\\nThere is no critical analysis of potential downsides, alternative approaches, or dissenting expert opinions. The language is consistently laudatory towards the government\\'s actions.'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = parallel_chain.invoke(\n",
    "    {\n",
    "        \"article\": \"\"\"\n",
    "The government's new infrastructure development program has received widespread acclaim for boosting economic growth and creating thousands of jobs across the country. \n",
    "Experts say the plan will improve transportation, connectivity, and local businesses, strengthening the nation's economy. \n",
    "Opposition parties have raised minor concerns, but the majority of citizens have welcomed the initiative, praising the government for taking decisive steps toward modernization and national progress.\n",
    "\"\"\"\n",
    "    }\n",
    ")\n",
    "# print(response)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09c5cad",
   "metadata": {},
   "source": [
    "### 3. Job Application Analyzer\n",
    "#### Tags: RunnableSequence, RunnableParallel\n",
    "#### Task:\n",
    "- Sequence: Parse job description -> extract skills.\n",
    "- Parallel: Run “skill match score” and “recommend missing skills” at the same time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53dcab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \" You are the job description analyzer. To analyse a skills.\"),\n",
    "        (\"human\", \"{job_description}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "job_description_chain = job_description_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6d76828",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_match_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a career assistant. Compare the candidate's skills with the job's required skills. Compute a skill match percentage (0-100%) indicating how well the candidate matches the job requirements.\"),\n",
    "        (\"human\", \"Candidate skill: {candidate_skills}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa7f6013",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_match_chain = skills_match_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ecd1f870",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_missing_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a career advisor. Identify skills required by the job that the candidate does not have. Return a list of missing skills.\"),\n",
    "        (\"human\", \"Candidate skill: {candidate_skills}\")\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee892131",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_missing_chain = skills_match_chain | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16204728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  skill_score: ChatPromptTemplate(input_variables=['candidate_skills'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"You are a career assistant. Compare the candidate's skills with the job's required skills. Compute a skill match percentage (0-100%) indicating how well the candidate matches the job requirements.\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['candidate_skills'], input_types={}, partial_variables={}, template='Candidate skill: {candidate_skills}'), additional_kwargs={})])\n",
       "               | ChatGoogleGenerativeAI(model='models/gemini-2.5-flash-lite', google_api_key=SecretStr('**********'), temperature=0.0, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x00000231609DDD30>, default_metadata=(), model_kwargs={})\n",
       "               | StrOutputParser(),\n",
       "  skill_missing: ChatPromptTemplate(input_variables=['candidate_skills'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"You are a career assistant. Compare the candidate's skills with the job's required skills. Compute a skill match percentage (0-100%) indicating how well the candidate matches the job requirements.\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['candidate_skills'], input_types={}, partial_variables={}, template='Candidate skill: {candidate_skills}'), additional_kwargs={})])\n",
       "                 | ChatGoogleGenerativeAI(model='models/gemini-2.5-flash-lite', google_api_key=SecretStr('**********'), temperature=0.0, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x00000231609DDD30>, default_metadata=(), model_kwargs={})\n",
       "                 | StrOutputParser()\n",
       "                 | ChatGoogleGenerativeAI(model='models/gemini-2.5-flash-lite', google_api_key=SecretStr('**********'), temperature=0.0, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x00000231609DDD30>, default_metadata=(), model_kwargs={})\n",
       "                 | StrOutputParser()\n",
       "}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain = RunnableParallel(\n",
    "    {\n",
    "        \"skill_score\": skills_match_chain,\n",
    "        \"skill_missing\": skills_missing_chain\n",
    "    }\n",
    ")\n",
    "final_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e3b0cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = job_description_chain.invoke(\n",
    "#     {\n",
    "#         \"job_description\": \"\"\"\n",
    "#     Position: Software Engineer  \n",
    "\n",
    "# Responsibilities:\n",
    "# - Develop and maintain web applications using Python and Django.\n",
    "# - Collaborate with cross-functional teams to design scalable solutions.\n",
    "# - Write clean, testable, and efficient code.\n",
    "\n",
    "# Requirements:\n",
    "# - Strong experience with Python and Django.\n",
    "# - Familiarity with REST APIs, PostgreSQL, and Git.\n",
    "# - Knowledge of Docker and Kubernetes is a plus.\n",
    "# - Good problem-solving and communication skills.\n",
    "\n",
    "#         \"\"\"\n",
    "#     }\n",
    "# )\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f941ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = final_chain.invoke(\n",
    "    {\n",
    "        \"job_description\": \"\"\"\n",
    "    Position: Software Engineer  \n",
    "\n",
    "Responsibilities:\n",
    "- Develop and maintain web applications using Python and Django.\n",
    "- Collaborate with cross-functional teams to design scalable solutions.\n",
    "- Write clean, testable, and efficient code.\n",
    "\n",
    "Requirements:\n",
    "- Strong experience with Python and Django.\n",
    "- Familiarity with REST APIs, PostgreSQL, and Git.\n",
    "- Knowledge of Docker and Kubernetes is a plus.\n",
    "- Good problem-solving and communication skills.\n",
    "\n",
    "        \"\"\",\n",
    "        \"candidate_skills\": \"\"\"\n",
    "        Name: Alex Johnson  \n",
    "\n",
    "Skills:\n",
    "- Python, Django, Flask\n",
    "- REST APIs, MySQL\n",
    "- Git, Linux\n",
    "- Problem-solving, teamwork, communication\n",
    "- Basic knowledge of Docker\n",
    "\"\"\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f0d8230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'skill_score': 'Job required skill:\\n        Job Title: Junior Python Developer\\n\\nSkills:\\n- Python, Django, Flask\\n- REST APIs, PostgreSQL\\n- Git, Docker\\n- Problem-solving, teamwork, communication\\n- Experience with cloud platforms (AWS, Azure, GCP)\\n- Unit testing', 'skill_missing': 'Here\\'s a breakdown of the skills required for a Junior Python Developer, categorized for clarity and with explanations of why each is important:\\n\\n## Junior Python Developer - Required Skills\\n\\n**Core Programming & Frameworks:**\\n\\n*   **Python:**\\n    *   **Why it\\'s essential:** This is the foundational language. You need a solid understanding of Python\\'s syntax, data structures (lists, dictionaries, tuples, sets), control flow (if/else, loops), functions, object-oriented programming (classes, inheritance), and common libraries.\\n*   **Django:**\\n    *   **Why it\\'s important:** A high-level, \"batteries-included\" Python web framework. Experience with Django means you can build web applications efficiently, understanding its ORM (Object-Relational Mapper), templating engine, routing, and administrative interface.\\n*   **Flask:**\\n    *   **Why it\\'s important:** A lightweight and flexible Python web framework. Knowing Flask demonstrates your ability to build web applications with more control and less boilerplate, often used for smaller projects or microservices.\\n\\n**Data & APIs:**\\n\\n*   **REST APIs:**\\n    *   **Why it\\'s essential:** Understanding how to design, build, and consume RESTful APIs is crucial for modern web development. This involves knowledge of HTTP methods (GET, POST, PUT, DELETE), status codes, JSON data format, and API design principles.\\n*   **PostgreSQL:**\\n    *   **Why it\\'s important:** A powerful and popular open-source relational database. You\\'ll need to know how to interact with databases, write SQL queries, understand database schema design, and potentially use an ORM (like Django\\'s) to manage data.\\n\\n**Development Tools & Practices:**\\n\\n*   **Git:**\\n    *   **Why it\\'s essential:** Version control is non-negotiable. You must be proficient in using Git for tracking code changes, collaborating with others, branching, merging, and resolving conflicts.\\n*   **Docker:**\\n    *   **Why it\\'s important:** Containerization is a standard practice for deploying and managing applications. Understanding Docker allows you to package your applications and their dependencies into portable containers, ensuring consistency across different environments.\\n\\n**Soft Skills & Problem Solving:**\\n\\n*   **Problem-solving:**\\n    *   **Why it\\'s essential:** Developers are essentially problem solvers. You need to be able to analyze issues, break them down into smaller parts, and devise effective solutions.\\n*   **Teamwork:**\\n    *   **Why it\\'s important:** Software development is rarely a solo effort. You\\'ll be working with other developers, designers, and project managers, so the ability to collaborate effectively is key.\\n*   **Communication:**\\n    *   **Why it\\'s important:** Clearly articulating your ideas, understanding requirements, and providing updates are vital for successful project execution. This includes both written and verbal communication.\\n\\n**Cloud & Deployment:**\\n\\n*   **Experience with cloud platforms (AWS, Azure, GCP):**\\n    *   **Why it\\'s important:** Many applications are deployed on cloud infrastructure. Familiarity with at least one major cloud provider means you understand concepts like virtual machines, storage, databases, and deployment services. Even basic knowledge is valuable for a junior role.\\n\\n**Quality Assurance:**\\n\\n*   **Unit testing:**\\n    *   **Why it\\'s essential:** Writing tests for your code ensures its correctness and helps prevent regressions. You should be familiar with writing unit tests to verify the functionality of individual components of your application.\\n\\n**In summary, a Junior Python Developer is expected to have a solid foundation in Python and its web frameworks, understand how to work with data and APIs, be proficient with essential development tools, possess strong problem-solving and collaboration skills, and have some exposure to cloud environments and testing practices.**'}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16144032",
   "metadata": {},
   "source": [
    "### 4. Multi-Model Response Generator\n",
    "#### Tags: RunnableParallel, RunnableLambda\n",
    "#### Task:\n",
    "Send the same user query to two different LLMs (e.g., GPT & Cohere) in parallel, then combine their responses into a single summarized answer using a lambda step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7cf0fc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm1 = ChatCohere(cohere_api_key = settings.cohere_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d47f348",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_chatprompt1 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are the helpful assistant to user aked the question.\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "06c52317",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_chat_chain = multi_chatprompt1 | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ab6b2b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_chatprompt2 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"you are the hlpfull assistant for user asked question\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b5215903",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_chat_chain2 = multi_chatprompt2 | llm1 | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0cc2c930",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_chain: RunnableParallel = RunnableParallel(\n",
    "    {\n",
    "        \"summary1\" : multi_chat_chain,\n",
    "        \"summary2\": multi_chat_chain2\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6d63a14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "007057d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summary1': \"Here's LLM in two points:\\n\\n1.  **LLM stands for Large Language Model.** It's a type of artificial intelligence designed to understand, generate, and process human language.\\n2.  **They are trained on massive amounts of text data.** This extensive training allows them to learn patterns, grammar, facts, and reasoning abilities, enabling them to perform a wide range of language-related tasks.\", 'summary2': \"Here are 2 key points about **LLM (Large Language Model)**:\\n\\n1. **Definition**:  \\n   An LLM is a type of artificial intelligence (AI) model designed to understand, generate, and manipulate human language. It is trained on vast amounts of text data to perform tasks like answering questions, writing text, translating languages, and more.\\n\\n2. **Key Features**:  \\n   LLMs are characterized by their large size (billions of parameters), deep learning architecture (often based on transformers), and ability to generalize across diverse language tasks without task-specific training. Examples include OpenAI's GPT, Google's Bard, and Meta's LLaMA.\"}\n"
     ]
    }
   ],
   "source": [
    "response = parallel_chain.invoke(\"what is LLM in 2 points\")\n",
    "# print(json.dumps(response, indent=4))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907fc928",
   "metadata": {},
   "source": [
    "### 5. Customer Support Ticket Classifier\n",
    "#### Tags: RunnableSequence\n",
    "#### Task:\n",
    "1. Preprocess text (remove PII) via lambda.\n",
    "2. Classify into department (billing, tech support, general).\n",
    "3. Route to appropriate handler function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5165f729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8d331d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "827bc872",
   "metadata": {},
   "source": [
    "### 6. Travel Planner with Dual Output\n",
    "#### Tags: RunnableParallel, RunnableLambda\n",
    "#### Task: Given a trip request, run in parallel:\n",
    "- Generate a budget-friendly itinerary.\n",
    "- Generate a luxury itinerary.\n",
    "Then lambda step: Merge them into a “compare & choose” format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250c7267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d716c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61eef67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ffcef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95503ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba9e29e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1600911b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0245206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463ff322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5349551",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "6. Travel Planner with Dual Output\n",
    "Tags: RunnableParallel, RunnableLambda\n",
    "Task: Given a trip request, run in parallel:\n",
    "- Generate a budget-friendly itinerary.\n",
    "- Generate a luxury itinerary.\n",
    "Then lambda step: Merge them into a “compare & choose” format.\n",
    "\n",
    "7. FAQ Bot with Contextual Lookup\n",
    "Tags: RunnableSequence, RunnableLambda\n",
    "Task:\n",
    "- Take user query -> embed & retrieve relevant docs from vector store -> pass to LLM for answer -> lambda step to add source citations.\n",
    "\n",
    "8. Real-Time Social Media Monitor\n",
    "Tags: RunnableParallel\n",
    "Task: Given a batch of tweets:\n",
    "- In parallel, run sentiment analysis, topic classification, and spam detection.\n",
    "- Aggregate results into a dashboard-friendly JSON output.\n",
    "\n",
    "9. Multi-Language Content Generator\n",
    "Tags: RunnableSequence, RunnableParallel\n",
    "Task:\n",
    "- Sequence: Take a topic -> generate English content -> Parallel: translate into French, Spanish, Hindi at the same time.\n",
    "\n",
    "10. AI Interview Assistant\n",
    "Tags: RunnableSequence, RunnableParallel, RunnableLambda\n",
    "Task:\n",
    "- Sequence: Analyze candidate’s answer to a question -> evaluate soft skills score.\n",
    "- Parallel: Check grammar, tone, and content relevance.\n",
    "- Lambda: Combine all scores into a final report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
