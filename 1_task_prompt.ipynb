{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adb1677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere import ChatCohere\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder,PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150856d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_settings import BaseSettings\n",
    "\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    cohere_api_key: str\n",
    "    \n",
    "    class Config:\n",
    "        env_file = \"../.env\"\n",
    "        extra = \"ignore\"\n",
    "        \n",
    "        \n",
    "settings = Settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f83d0d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatCohere(cohere_api_key = settings.cohere_api_key, temperature=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf74fef6",
   "metadata": {},
   "source": [
    "# Task 1: Static Conversation Construction\n",
    "# Tags: SystemMessage, HumanMessage, AIMessage\n",
    "# Task: Create a static list of messages to simulate a basic assistant conversation. Print the messages and discuss the message roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e56d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Kerala is Thiruvananthapuram.\n"
     ]
    }
   ],
   "source": [
    "static_list = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content = \"what is the capital of Tamil Nadu?\"),\n",
    "    AIMessage(content= \"The capital of Tamil Nadu is Chennai.\"),\n",
    "    HumanMessage(content = \"What is the capital of Kerala?\"),\n",
    "]\n",
    "print(llm.invoke(static_list).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30c1db3",
   "metadata": {},
   "source": [
    "# Task 2: Reusable Prompt Template\n",
    "# Tags: ChatPromptTemplate, format_messages()\n",
    "# Task: Create a prompt template with variables like {name} or {topic}. Format the prompt with different values and observe the structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4572f47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['height', 'name', 'user_input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], input_types={}, partial_variables={}, template='You are a helpful assistent. Your name is {name}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='What is the height of Everest Mountain'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['height'], input_types={}, partial_variables={}, template='The Everest Mountain height is {height}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, template='{user_input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistent. Your name is {name}\"),\n",
    "        (\"human\", \"What is the height of Everest Mountain\"),\n",
    "        (\"ai\", \"The Everest Mountain height is {height}\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    "    \n",
    ")  \n",
    "chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59f7ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='what is the capital of Tamil Nadu?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='The capital of Tamil Nadu is Chennai.', additional_kwargs={}, response_metadata={}),\n",
       " SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is the capital of Kerala?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# template = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         HumanMessage(content = \"what is the capital of Tamil Nadu?\"),\n",
    "#         AIMessage(content= \"The capital of Tamil Nadu is Chennai.\"),\n",
    "#         SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "#         HumanMessage(content = \"What is the capital of Kerala?\"),    \n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a017463d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistent. Your name is Riya', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is the height of Everest Mountain', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='The Everest Mountain height is 884886', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what is the height of K2?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = chat_template.invoke(\n",
    "    {\n",
    "        \"name\" : \"Riya\",\n",
    "        \"height\" : 884886,\n",
    "        \"user_input\" : \"what is the height of K2?\"\n",
    "    }\n",
    ")\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "626e7b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'K2, also known as Mount Godwin Austen, is the second-highest mountain in the world. Its height is **8,611 meters (28,251 feet)** above sea level. It is located on the border between Pakistan and China in the Karakoram Range. K2 is often considered one of the most challenging mountains to climb due to its steep slopes and unpredictable weather conditions.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(prompt)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c28eed",
   "metadata": {},
   "source": [
    "# Task 3: Inject Memory into Prompt\n",
    "# Tags: MessagesPlaceholder, ChatPromptTemplate\n",
    "# Task: Create a prompt that includes a placeholder for chat history. Insert sample conversation turns into the placeholder and format the final prompt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c8a183a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['message', 'question'], input_types={'message': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000002A56CF780E0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='you are a helpful assistant'), additional_kwargs={}), MessagesPlaceholder(variable_name='message'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"you are a helpful assistant\"),\n",
    "    MessagesPlaceholder(\"message\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "final_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98e8b423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='you are a helpful assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is the division of 6/2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='the division is 3', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what is the multiply of 4 * 3?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prompt.invoke(\n",
    "    {\n",
    "        \"message\" : [(\"human\" , \"What is the division of 6/2\"), (\"ai\", \"the division is 3\")],\n",
    "            \n",
    "        \"question\": \"what is the multiply of 4 * 3?\"\n",
    "    }\n",
    ").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65f75183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The multiplication of 4 * 3 is **12**.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(final_prompt.invoke(\n",
    "    {\n",
    "        \"message\" : [(\"human\" , \"What is the division of 6/2\"), (\"ai\", \"the division is 3\")],\n",
    "            \n",
    "        \"question\": \"what is the multiply of 4 * 3?\"\n",
    "    }\n",
    ").to_messages()).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65b1d74",
   "metadata": {},
   "source": [
    "# Task 5: Role Experimentation\n",
    "# Tags: SystemMessage\n",
    "# Task: Change the behavior of the assistant by modifying the system message. Try roles like \"sarcastic\", \"cheerful\", or \"strict tutor\" and note changes in output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0895cd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, really? Praveen loves Riya, huh? Well, that’s just *groundbreaking* news. I mean, who would’ve guessed that someone could have feelings for another person? It’s not like that’s a common human experience or anything. \n",
      "\n",
      "Here’s a helpful tip for you: If you’re so sure about Praveen’s feelings, maybe just ask him directly instead of speculating. Or better yet, mind your own business and let them figure it out. Love is complicated enough without random people inserting their opinions. \n",
      "\n",
      "But hey, thanks for sharing. I’m sure Praveen and Riya are *thrilled* that their personal lives are now a topic of discussion. Keep up the detective work! 🕵️‍♂️💖\n"
     ]
    }
   ],
   "source": [
    "message = [\n",
    "    SystemMessage(content = \"You are a sarcastic guy who answer all the question in a sarcastic way.\"),\n",
    "    SystemMessage(content = \"You are helpful guy who assist with some helpful tips about the particular question.\"),\n",
    "    HumanMessage(content = \"I think Praveen loves Riya.\"),\n",
    "]\n",
    "print(llm.invoke(message).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ec59b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I don't have personal knowledge or access to private information about individuals, so I can't confirm or deny whether Praveen loves Riya. Relationships and feelings are personal matters that depend on the individuals involved. If you have specific questions or need advice about relationships, I’d be happy to help in a general sense!\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"I think Praveen loves Riya\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f963eb",
   "metadata": {},
   "source": [
    "# Task 6: Message Ordering Impact\n",
    "# Tags: SystemMessage, HumanMessage, AIMessage\n",
    "# Task: Rearrange the order of messages and observe how the model's response changes. Discuss why message order is important.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1cb4f9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is the height of Everest Mountain', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='The Everest Mountain height is 100', additional_kwargs={}, response_metadata={}),\n",
       " SystemMessage(content='You are a helpful assistent', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate(\n",
    "    [\n",
    "        \n",
    "        (\"human\", \"What is the height of Everest Mountain\"),\n",
    "        (\"ai\", \"The Everest Mountain height is 100\"),\n",
    "        (\"system\", \"You are a helpful assistent\"),\n",
    "        \n",
    "    ]\n",
    "    \n",
    ")\n",
    "chat_template.invoke({}).messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e80feea2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The last message is not an ToolMessage or HumanMessage",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m response.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VRNeXGen\\my_ai_env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:372\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    362\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    367\u001b[39m     **kwargs: Any,\n\u001b[32m    368\u001b[39m ) -> BaseMessage:\n\u001b[32m    369\u001b[39m     config = ensure_config(config)\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    371\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    382\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VRNeXGen\\my_ai_env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:957\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    948\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    950\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    954\u001b[39m     **kwargs: Any,\n\u001b[32m    955\u001b[39m ) -> LLMResult:\n\u001b[32m    956\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VRNeXGen\\my_ai_env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:776\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    775\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m         )\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    784\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VRNeXGen\\my_ai_env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1022\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1020\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VRNeXGen\\my_ai_env\\Lib\\site-packages\\langchain_cohere\\chat_models.py:1136\u001b[39m, in \u001b[36mChatCohere._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1131\u001b[39m     stream_iter = \u001b[38;5;28mself\u001b[39m._stream(\n\u001b[32m   1132\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1133\u001b[39m     )\n\u001b[32m   1134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m generate_from_stream(stream_iter)\n\u001b[32m-> \u001b[39m\u001b[32m1136\u001b[39m request = \u001b[43mget_cohere_chat_request_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_default_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m response = \u001b[38;5;28mself\u001b[39m.client.v2.chat(**request)\n\u001b[32m   1141\u001b[39m generation_info = \u001b[38;5;28mself\u001b[39m._get_generation_info_v2(\n\u001b[32m   1142\u001b[39m     response, request.get(\u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1143\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VRNeXGen\\my_ai_env\\Lib\\site-packages\\langchain_cohere\\chat_models.py:535\u001b[39m, in \u001b[36mget_cohere_chat_request_v2\u001b[39m\u001b[34m(messages, documents, stop_sequences, **kwargs)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;66;03m# check if the last message is a tool message or human message\u001b[39;00m\n\u001b[32m    532\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[32m    533\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(messages[-\u001b[32m1\u001b[39m], ToolMessage) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(messages[-\u001b[32m1\u001b[39m], HumanMessage)\n\u001b[32m    534\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThe last message is not an ToolMessage or HumanMessage\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpreamble\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    538\u001b[39m     messages = [SystemMessage(content=\u001b[38;5;28mstr\u001b[39m(kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpreamble\u001b[39m\u001b[33m\"\u001b[39m)))] + messages\n",
      "\u001b[31mValueError\u001b[39m: The last message is not an ToolMessage or HumanMessage"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(chat_template.invoke({}).messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9c4efbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is the height of Everest Mountain', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='The Everest Mountain height is 100', additional_kwargs={}, response_metadata={}),\n",
       " SystemMessage(content='You are a helpful assistent', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what is the height of Mountain K2 ?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate(\n",
    "    [\n",
    "        \n",
    "        (\"human\", \"What is the height of Everest Mountain\"),\n",
    "        (\"ai\", \"The Everest Mountain height is 100\"),\n",
    "        (\"system\", \"You are a helpful assistent\"),\n",
    "        (\"human\", \"what is the height of Mountain K2 ?\")\n",
    "        \n",
    "    ]\n",
    "    \n",
    ")\n",
    "chat_template.invoke({}).messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ccb14e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The height of K2, the second-highest mountain in the world, is **8,611 meters (28,251 feet)** above sea level. It is located in the Karakoram Range, on the border between Pakistan and China. K2 is often considered one of the most challenging mountains to climb due to its steep slopes and unpredictable weather conditions.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(chat_template.invoke({}).messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc60b297",
   "metadata": {},
   "source": [
    "# Task 7: Dynamic Prompt Debugging\n",
    "# Tags: ChatPromptTemplate, format_messages()\n",
    "# Task: Intentionally leave out or mislabel a variable in a prompt template. Fix the issue and explain the error trace.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a6511013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=[], input_types={}, partial_variables={}, messages=[SystemMessage(content='You are a helpful assistant. your name is {name}', additional_kwargs={}, response_metadata={}), HumanMessage(content='what is your name?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content = \"You are a helpful assistant. your name is {name}\"),\n",
    "        HumanMessage(content = \"what is your name?\")\n",
    "    ]\n",
    ")\n",
    "dynamic_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c59a8f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83c3acdb",
   "metadata": {},
   "source": [
    "# Task 8: Personalized Assistant\n",
    "# Tags: ChatPromptTemplate, SystemMessage, MessagesPlaceholder\n",
    "# Task: Create a prompt template for a personalized assistant that remembers a user's preferences and responds accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5b1b326d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['message', 'question'], input_types={'message': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x00000206FD398180>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='you are my personal assistant. you remember s user preference and responds '), additional_kwargs={}), MessagesPlaceholder(variable_name='message'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"you are my personal assistant. you remember s user preference and responds \"),\n",
    "        MessagesPlaceholder(\"message\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]    \n",
    ")\n",
    "chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "29e64c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = chat_template.invoke(\n",
    "    {\n",
    "        \"message\": [(\"human\", \"what is python\"), (\"ai\", \"Python is an interpreted, object-oriented, high-level programming language with dynamic semantics.\")],\n",
    "        \"question\" : \"write short notes on AI ML Engineer?\"\n",
    "}\n",
    ").messages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d100ba55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**AI/ML Engineer: Short Notes**\\n\\nAn **AI/ML (Artificial Intelligence/Machine Learning) Engineer** is a specialized professional who designs, develops, and deploys AI and machine learning systems to solve complex problems. Here are the key aspects of their role:\\n\\n1. **Responsibilities**:  \\n   - Build and train machine learning models using algorithms like regression, classification, clustering, and deep learning.  \\n   - Preprocess and clean large datasets for model training.  \\n   - Optimize models for performance, scalability, and accuracy.  \\n   - Deploy AI/ML solutions into production environments.  \\n   - Collaborate with data scientists, software engineers, and stakeholders to integrate AI into applications.  \\n\\n2. **Skills Required**:  \\n   - Proficiency in programming languages like Python, R, or Java.  \\n   - Knowledge of ML frameworks (TensorFlow, PyTorch, Scikit-learn) and libraries.  \\n   - Understanding of data structures, algorithms, and statistical modeling.  \\n   - Experience with cloud platforms (AWS, GCP, Azure) for model deployment.  \\n   - Strong problem-solving and analytical skills.  \\n\\n3. **Tools and Technologies**:  \\n   - Data processing tools: Pandas, NumPy, Spark.  \\n   - Model deployment: Docker, Kubernetes, Flask/FastAPI.  \\n   - Visualization: Matplotlib, Seaborn, Tableau.  \\n\\n4. **Applications**:  \\n   - Natural Language Processing (NLP), computer vision, recommendation systems, predictive analytics, and automation.  \\n\\n5. **Career Path**:  \\n   - Typically requires a background in computer science, mathematics, or a related field.  \\n   - Roles can range from junior ML engineer to senior AI architect, depending on experience.  \\n\\nAI/ML engineers play a crucial role in bridging the gap between theoretical AI models and real-world applications, driving innovation across industries.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(prompt)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4de417",
   "metadata": {},
   "source": [
    "# Task 9: Multiple Placeholders Use Case\n",
    "# Tags: ChatPromptTemplate, MessagesPlaceholder\n",
    "# Task: Create a template with two different MessagesPlaceholder blocks—one for history, one for external tool responses. Explain how each is used in a real-world use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "25378226",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant\"),\n",
    "        MessagesPlaceholder(\"history\"),\n",
    "        MessagesPlaceholder(\"tool_response\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "45e3af05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is the capital of India', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='The capital of India is New Delhi', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='How is the weather today?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='It’s sunny and warm today.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Who is the President of India?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple.invoke(\n",
    "    {\n",
    "        \"history\" : [(\"human\", \"What is the capital of India\"), (\"ai\", \"The capital of India is New Delhi\")],\n",
    "        \"tool_response\" : [(\"human\", \"How is the weather today?\"), (\"ai\", \"It’s sunny and warm today.\")],\n",
    "        \"question\" : \"Who is the President of India?\"\n",
    "    }\n",
    ").messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "938c4719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As of October 2023, the President of India is **Droupadi Murmu**. She assumed office on July 25, 2022, as the 15th President of India.'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(\n",
    "    multiple.invoke(\n",
    "    {\n",
    "        \"history\" : [(\"human\", \"What is the capital of India\"), (\"ai\", \"The capital of India is New Delhi\")],\n",
    "        \"tool_response\" : [(\"human\", \"How is the weather today?\"), (\"ai\", \"It’s sunny and warm today.\")],\n",
    "        \"question\" : \"Who is the President of India?\"\n",
    "    }\n",
    ").messages\n",
    "\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0d8b7bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The question of whether a Prime Minister (PM) or a President is superior or has more power depends on the **political system** of the country in question. The roles, authority, and responsibilities of a PM and a President vary significantly between **parliamentary systems** (common in countries like the UK, India, and Canada) and **presidential systems** (common in the US, France under the semi-presidential system, and others). Here's a breakdown:\\n\\n### **1. Parliamentary Systems (Prime Minister-led)**\\n- **Prime Minister**: The PM is typically the head of government and holds the most executive power. They are usually the leader of the majority party in the legislature and are accountable to the parliament.\\n- **President (if present)**: In many parliamentary systems, the President is a ceremonial or symbolic figurehead with limited or no executive power (e.g., India, Ireland, Israel).\\n- **Power Dynamics**: The PM has more authority in day-to-day governance, policy-making, and administration.\\n\\n### **2. Presidential Systems (President-led)**\\n- **President**: The President is both the head of state and the head of government, holding significant executive power. They are directly elected by the people or an electoral college.\\n- **Prime Minister (if present)**: In some presidential systems, a PM may exist but serves at the discretion of the President and has limited independent authority (e.g., South Korea, Russia in some periods).\\n- **Power Dynamics**: The President has more power, including control over foreign policy, military decisions, and appointments.\\n\\n### **3. Semi-Presidential Systems (Mixed)**\\n- **Both President and Prime Minister**: In these systems (e.g., France, Russia), power is shared between the President and the PM. The President often handles foreign policy and defense, while the PM manages domestic affairs.\\n- **Power Dynamics**: The balance of power depends on the constitution and political context. In France, for example, the President has more authority, especially in times of political stability.\\n\\n### **Key Factors Determining Power:**\\n1. **Constitutional Framework**: The constitution defines the roles and powers of each position.\\n2. **Political Context**: The strength of the ruling party, public support, and the relationship between the PM and President (if both exist) influence power dynamics.\\n3. **Electoral Process**: Directly elected Presidents often have a stronger mandate and legitimacy compared to PMs, who are typically appointed by the legislature.\\n\\n### **Conclusion:**\\n- In **parliamentary systems**, the **Prime Minister** is generally more powerful.\\n- In **presidential systems**, the **President** holds more authority.\\n- In **semi-presidential systems**, power is shared, and the superior position depends on the specific constitutional arrangement.\\n\\nThere is no universal answer; it depends entirely on the country's political structure.\", additional_kwargs={'id': '87bc9353-c243-4b7b-8aed-25e4754ce1c9', 'finish_reason': 'COMPLETE', 'content': \"The question of whether a Prime Minister (PM) or a President is superior or has more power depends on the **political system** of the country in question. The roles, authority, and responsibilities of a PM and a President vary significantly between **parliamentary systems** (common in countries like the UK, India, and Canada) and **presidential systems** (common in the US, France under the semi-presidential system, and others). Here's a breakdown:\\n\\n### **1. Parliamentary Systems (Prime Minister-led)**\\n- **Prime Minister**: The PM is typically the head of government and holds the most executive power. They are usually the leader of the majority party in the legislature and are accountable to the parliament.\\n- **President (if present)**: In many parliamentary systems, the President is a ceremonial or symbolic figurehead with limited or no executive power (e.g., India, Ireland, Israel).\\n- **Power Dynamics**: The PM has more authority in day-to-day governance, policy-making, and administration.\\n\\n### **2. Presidential Systems (President-led)**\\n- **President**: The President is both the head of state and the head of government, holding significant executive power. They are directly elected by the people or an electoral college.\\n- **Prime Minister (if present)**: In some presidential systems, a PM may exist but serves at the discretion of the President and has limited independent authority (e.g., South Korea, Russia in some periods).\\n- **Power Dynamics**: The President has more power, including control over foreign policy, military decisions, and appointments.\\n\\n### **3. Semi-Presidential Systems (Mixed)**\\n- **Both President and Prime Minister**: In these systems (e.g., France, Russia), power is shared between the President and the PM. The President often handles foreign policy and defense, while the PM manages domestic affairs.\\n- **Power Dynamics**: The balance of power depends on the constitution and political context. In France, for example, the President has more authority, especially in times of political stability.\\n\\n### **Key Factors Determining Power:**\\n1. **Constitutional Framework**: The constitution defines the roles and powers of each position.\\n2. **Political Context**: The strength of the ruling party, public support, and the relationship between the PM and President (if both exist) influence power dynamics.\\n3. **Electoral Process**: Directly elected Presidents often have a stronger mandate and legitimacy compared to PMs, who are typically appointed by the legislature.\\n\\n### **Conclusion:**\\n- In **parliamentary systems**, the **Prime Minister** is generally more powerful.\\n- In **presidential systems**, the **President** holds more authority.\\n- In **semi-presidential systems**, power is shared, and the superior position depends on the specific constitutional arrangement.\\n\\nThere is no universal answer; it depends entirely on the country's political structure.\", 'token_count': {'input_tokens': 507.0, 'output_tokens': 607.0}}, response_metadata={'id': '87bc9353-c243-4b7b-8aed-25e4754ce1c9', 'finish_reason': 'COMPLETE', 'content': \"The question of whether a Prime Minister (PM) or a President is superior or has more power depends on the **political system** of the country in question. The roles, authority, and responsibilities of a PM and a President vary significantly between **parliamentary systems** (common in countries like the UK, India, and Canada) and **presidential systems** (common in the US, France under the semi-presidential system, and others). Here's a breakdown:\\n\\n### **1. Parliamentary Systems (Prime Minister-led)**\\n- **Prime Minister**: The PM is typically the head of government and holds the most executive power. They are usually the leader of the majority party in the legislature and are accountable to the parliament.\\n- **President (if present)**: In many parliamentary systems, the President is a ceremonial or symbolic figurehead with limited or no executive power (e.g., India, Ireland, Israel).\\n- **Power Dynamics**: The PM has more authority in day-to-day governance, policy-making, and administration.\\n\\n### **2. Presidential Systems (President-led)**\\n- **President**: The President is both the head of state and the head of government, holding significant executive power. They are directly elected by the people or an electoral college.\\n- **Prime Minister (if present)**: In some presidential systems, a PM may exist but serves at the discretion of the President and has limited independent authority (e.g., South Korea, Russia in some periods).\\n- **Power Dynamics**: The President has more power, including control over foreign policy, military decisions, and appointments.\\n\\n### **3. Semi-Presidential Systems (Mixed)**\\n- **Both President and Prime Minister**: In these systems (e.g., France, Russia), power is shared between the President and the PM. The President often handles foreign policy and defense, while the PM manages domestic affairs.\\n- **Power Dynamics**: The balance of power depends on the constitution and political context. In France, for example, the President has more authority, especially in times of political stability.\\n\\n### **Key Factors Determining Power:**\\n1. **Constitutional Framework**: The constitution defines the roles and powers of each position.\\n2. **Political Context**: The strength of the ruling party, public support, and the relationship between the PM and President (if both exist) influence power dynamics.\\n3. **Electoral Process**: Directly elected Presidents often have a stronger mandate and legitimacy compared to PMs, who are typically appointed by the legislature.\\n\\n### **Conclusion:**\\n- In **parliamentary systems**, the **Prime Minister** is generally more powerful.\\n- In **presidential systems**, the **President** holds more authority.\\n- In **semi-presidential systems**, power is shared, and the superior position depends on the specific constitutional arrangement.\\n\\nThere is no universal answer; it depends entirely on the country's political structure.\", 'token_count': {'input_tokens': 507.0, 'output_tokens': 607.0}}, id='run--462d2a72-5ea7-4d15-a1ee-1264c4105f58-0', usage_metadata={'input_tokens': 507, 'output_tokens': 607, 'total_tokens': 1114})"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"PM or President who is superior? who has more power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df2b814",
   "metadata": {},
   "source": [
    "# Task 10: Compare Prompt vs Message Interface\n",
    "# Tags: PromptTemplate, ChatPromptTemplate\n",
    "# Task: Create one prompt using PromptTemplate (text-only) and another using ChatPromptTemplate. Compare the outputs when passed to a chat model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f2c84f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='I love Python programming language')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variable = [\"programming_language\"],\n",
    "    template = \"I love {programming_language} programming language\"\n",
    ")\n",
    "prompt.invoke({\"programming_language\": \"Python\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4c3fde26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's fantastic! Python is indeed a wonderful programming language, known for its simplicity, readability, and versatility. It's widely used in various domains such as web development, data science, artificial intelligence, automation, and more. Here are a few reasons why many people, including yourself, love Python:\\n\\n1. **Easy to Learn and Use**: Python has a clean and intuitive syntax, making it accessible for beginners and efficient for experienced developers.\\n\\n2. **Large Standard Library**: Python comes with a comprehensive standard library that supports many common programming tasks, reducing the need to write additional code.\\n\\n3. **Vast Ecosystem of Libraries and Frameworks**: Whether you're into web development (Django, Flask), data science (Pandas, NumPy, Matplotlib), machine learning (TensorFlow, PyTorch), or automation (Selenium), there's a library or framework for almost everything.\\n\\n4. **Community Support**: Python has a large and active community. This means plenty of resources, tutorials, and forums where you can seek help or share your knowledge.\\n\\n5. **Cross-Platform Compatibility**: Python runs on various platforms including Windows, macOS, Linux, and even on embedded systems, making it highly versatile.\\n\\n6. **Dynamically Typed**: Python is dynamically typed, which allows for faster development and more flexibility, though it requires careful testing to ensure type correctness.\\n\\n7. **Open Source**: Python is open source, which means it's free to use and distribute, and you can even contribute to its development.\\n\\nIf you have any specific questions about Python or need help with a particular project, feel free to ask! What kind of projects are you working on or interested in exploring with Python?\""
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(prompt.invoke({\"programming_language\": \"Python\"}))\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40442c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love Python programming language\n"
     ]
    }
   ],
   "source": [
    "# formatted = prompt.format(**{\"programming_language\":\"Python\"})\n",
    "# print(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "bf573e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"I love {programming_language} programming language\")\n",
    "        \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1dc942e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's fantastic! Python is indeed a wonderful programming language, known for its simplicity, readability, and versatility. It's widely used in various domains such as web development, data science, artificial intelligence, automation, and more. Here are a few reasons why many people, including yourself, love Python:\\n\\n1. **Easy to Learn and Use**: Python has a clean and intuitive syntax, making it accessible for beginners and experienced developers alike.\\n\\n2. **Large Standard Library**: Python comes with a comprehensive standard library that supports many common programming tasks, reducing the need to write additional code.\\n\\n3. **Vast Ecosystem of Libraries and Frameworks**: Whether you're into web development (Django, Flask), data science (Pandas, NumPy, Matplotlib), machine learning (TensorFlow, PyTorch), or automation (Selenium), Python has a library or framework for almost everything.\\n\\n4. **Community Support**: Python has a large and active community. This means plenty of resources, tutorials, and forums where you can seek help or share knowledge.\\n\\n5. **Cross-Platform**: Python is available and runs on various operating systems, including Windows, macOS, and Linux, making it highly portable.\\n\\n6. **Dynamic Typing**: Python uses dynamic typing, which can make development faster and more flexible, though it requires careful testing to avoid type-related bugs.\\n\\n7. **Interoperability**: Python can be integrated with other languages and technologies, allowing you to leverage existing codebases or systems.\\n\\nIf you have any specific questions about Python or need help with a particular project, feel free to ask! What are you currently working on or interested in exploring further with Python?\""
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(chat_prompt.invoke({\"programming_language\": \"Python\"}))\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112871a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5695ac12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2e873a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistent.\"),\n",
    "        (\"human\", \"What is the National Animal of India?\"),\n",
    "        (\"ai\", \"The national animal of india is {animal}\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "39a5bfc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistent.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is the National Animal of India?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='The national animal of india is Bengal Tiger', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is the National Fruit?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt2 = chat_prompt.invoke(\n",
    "    {\n",
    "        \"animal\" : \"Bengal Tiger\",\n",
    "        \"user_input\" : \"What is the National Fruit?\"\n",
    "    }\n",
    ")\n",
    "prompt2.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "714e15da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The national fruit of India is the **Mango** (*Mangifera indica*). It is widely cultivated and cherished across the country for its delicious taste and cultural significance.'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(prompt2)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4712e871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ef9352d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
